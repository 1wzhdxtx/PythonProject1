import pymysql
import json
import requests
from bs4 import BeautifulSoup
from elasticsearch import Elasticsearch, helpers
from datetime import datetime


# æŠŠè¿™ä¸ªæ¢æˆä¸»æœºip
MYSQL_HOST = "192.168.1.10"
ES_HOST = "http://192.168.1.10:9200"

# MySQL è¿æ¥ä¿¡æ¯
user = input("è¯·è¾“å…¥ä½ çš„ MySQL ç”¨æˆ·å (é»˜è®¤: root): ") or "root"
pw = input("è¯·è¾“å…¥ä½ çš„ MySQL å¯†ç : ")
db = input("è¯·è¾“å…¥ä½ æƒ³ä½¿ç”¨çš„æ•°æ®åº“: ")


# Elasticsearch é…ç½®ï¼ˆå°±æ˜¯ç»™ç´¢å¼•èµ·åï¼‰
INDEX_NAME = "web_scraped_data"

# è¿æ¥ Elasticsearch
es = Elasticsearch([ES_HOST])

# åˆ›å»º Elasticsearch ç´¢å¼•
def create_index():
    mapping = {
        "mappings": {
            "properties": {
                "url": {"type": "keyword"},
                "title": {"type": "text", "analyzer": "standard"},
                "html": {"type": "text"},
                "timestamp": {"type": "date"}
            }
        }
    }
    if not es.indices.exists(index=INDEX_NAME):
        es.indices.create(index=INDEX_NAME, body=mapping)
        print(f"[âœ”] Elasticsearch ç´¢å¼• '{INDEX_NAME}' å·²åˆ›å»º")
    else:
        print(f"[âœ”] Elasticsearch ç´¢å¼• '{INDEX_NAME}' å·²å­˜åœ¨")

# è·å– HTML å†…å®¹ï¼Œè¿™æœ‰ä¸€ä¸ªurlå˜é‡éœ€è¦è¡”æ¥ä¸€ä¸‹
def get_html(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"[âœ˜] è·å– {url} å¤±è´¥: {e}")
        return None

# æå–ç½‘é¡µæ ‡é¢˜ï¼Œè¿™ä¹Ÿæœ‰ä¸ªå˜é‡
def extract_title(html_content):
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.title.string if soup.title else "æ— æ ‡é¢˜"

# è¿æ¥ MySQL
try:
    connection = pymysql.connect(
        host=MYSQL_HOST,
        port=3306,
        user=user,
        password=pw,
        database=db,
        charset='utf8mb4',
        autocommit=True
    )
    print("[âœ”] æˆåŠŸè¿æ¥åˆ° MySQL æ•°æ®åº“:", db)

    # è·å–å¾…çˆ¬å–çš„ URL åˆ—è¡¨
    with connection.cursor() as cursor:
        cursor.execute("SELECT url FROM web_scraped_urls")  # è¯»å–å¾…çˆ¬å–çš„ URL
        url_list = [row[0] for row in cursor.fetchall()]

    # å­˜å‚¨æ•°æ®
    data = []
    for url in url_list:
        html_content = get_html(url)
        if html_content:
            title = extract_title(html_content)
            extracted_data = json.dumps({"title": title})
            data.append((url, html_content, extracted_data))

    # æ’å…¥æ•°æ®åˆ° MySQL
    if data:
        with connection.cursor() as cursor:
            sql = "INSERT INTO web_scraped_data (url, html, extracted_data) VALUES (%s, %s, %s)"
            cursor.executemany(sql, data)
        print("[âœ”] æ•°æ®æˆåŠŸæ’å…¥ MySQL")

        # ç´¢å¼•åˆ° Elasticsearch
        es_data = [
            {
                "_index": INDEX_NAME,
                "_source": {
                    "url": url,
                    "title": json.loads(extracted_data)["title"],
                    "html": html_content,
                    "timestamp": datetime.utcnow().isoformat()
                }
            }
            for url, html, extracted_data in data
        ]
        helpers.bulk(es, es_data)
        print(f"[âœ”] {len(es_data)} æ¡æ•°æ®å·²ç´¢å¼•åˆ° Elasticsearch")

except pymysql.MySQLError as e:
    print("[âœ˜] MySQL é”™è¯¯:", e)

except Exception as e:
    print("[âœ˜] å‘ç”ŸæœªçŸ¥é”™è¯¯:", e)

finally:
    if 'connection' in locals() and connection.open:
        connection.close()
        print("[âœ”] æ•°æ®åº“è¿æ¥å·²å…³é—­")
# æœç´¢æ•°æ®éƒ¨åˆ†
keyword = input("è¯·è¾“å…¥è¦æœç´¢çš„å…³é”®å­—: ")
query = {"query": {"match": {"title": keyword}}}

try:
    results = es.search(index=INDEX_NAME, body=query, size=5)
    hits = results.get("hits", {}).get("hits", [])

    if hits:
        print("\nğŸ” æœç´¢ç»“æœ:")
        for hit in hits:
            print(f"URL: {hit['_source']['url']}, æ ‡é¢˜: {hit['_source']['title']}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
except Exception as e:
    print(f"[âœ˜] Elasticsearch æŸ¥è¯¢å¤±è´¥: {e}")